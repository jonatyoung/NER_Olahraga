{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        train = f.read().split(\"\\n\\n\")\n",
    "        train = [x.split(\"\\n\") for x in train]\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        sentence = train[i][0].split(\" \")\n",
    "        label = train[i][1].split(\" \")\n",
    "        if len(sentence) == len(label):\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 22:54:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c61b88fb26a44339dc2274cc3ea63db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 22:54:26 INFO: Downloaded file to C:\\Users\\jonat\\stanza_resources\\resources.json\n",
      "2024-11-27 22:54:26 WARNING: Language id package default expects mwt, which has been added\n",
      "2024-11-27 22:54:26 INFO: Loading these models for language: id (Indonesian):\n",
      "==========================\n",
      "| Processor | Package    |\n",
      "--------------------------\n",
      "| tokenize  | gsd        |\n",
      "| mwt       | gsd        |\n",
      "| pos       | gsd_charlm |\n",
      "==========================\n",
      "\n",
      "2024-11-27 22:54:26 INFO: Using device: cuda\n",
      "2024-11-27 22:54:26 INFO: Loading: tokenize\n",
      "c:\\Users\\jonat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-27 22:54:26 INFO: Loading: mwt\n",
      "c:\\Users\\jonat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-27 22:54:26 INFO: Loading: pos\n",
      "c:\\Users\\jonat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\jonat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "c:\\Users\\jonat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-27 22:54:26 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Stanza NLP pipeline\n",
    "nlp = stanza.Pipeline('id', processors='tokenize,pos')\n",
    "\n",
    "# Data Sample\n",
    "sentences, labels = extract_text(\"TAGGED REVISI DIKIT.txt\")\n",
    "\n",
    "\n",
    "# Create Vocabulary for words, POS tags, and labels\n",
    "word2idx = defaultdict(lambda: len(word2idx))\n",
    "pos2idx = defaultdict(lambda: len(pos2idx))\n",
    "label2idx = defaultdict(lambda: len(label2idx))\n",
    "\n",
    "# Add special tokens\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "pos2idx[\"<PAD>\"] = 0\n",
    "label2idx[\"O\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess training data for initial vocabulary\n",
    "def preprocess_training_data(sentences, labels):\n",
    "    processed_data = []\n",
    "    for sentence, sent_labels in zip(sentences, labels):\n",
    "        # Use Stanza for POS tagging\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "        \n",
    "        processed_words = []\n",
    "        processed_pos = []\n",
    "        processed_labels = []\n",
    "        \n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                processed_words.append(word.text)\n",
    "                processed_pos.append(word.upos)\n",
    "        \n",
    "        # Match labels to processed words (assuming same order)\n",
    "        processed_labels = sent_labels[:len(processed_words)]\n",
    "        \n",
    "        processed_data.append((processed_words, processed_pos, processed_labels))\n",
    "        \n",
    "        # Update vocabularies\n",
    "        for word in processed_words:\n",
    "            word2idx[word]\n",
    "        for pos in processed_pos:\n",
    "            pos2idx[pos]\n",
    "        for label in processed_labels:\n",
    "            label2idx[label]\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Preprocess the initial training data\n",
    "processed_training_data = preprocess_training_data(sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words, POS tags, and labels to indices\n",
    "input_words = [[word2idx[word] for word in sentence[0]] for sentence in processed_training_data]\n",
    "input_pos = [[pos2idx[pos] for pos in sentence[1]] for sentence in processed_training_data]\n",
    "label_data = [[label2idx[label] for label in sentence[2]] for sentence in processed_training_data]\n",
    "\n",
    "# Dynamic Padding: Use the length of the longest sentence in the data\n",
    "MAX_LEN = max([len(sentence) for sentence in input_words])\n",
    "\n",
    "# Padding the sequences\n",
    "input_words = [sentence + [0]*(MAX_LEN - len(sentence)) for sentence in input_words]\n",
    "input_pos = [pos + [0]*(MAX_LEN - len(pos)) for pos in input_pos]\n",
    "label_data = [label + [0]*(MAX_LEN - len(label)) for label in label_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, words, pos, labels):\n",
    "        self.words = words\n",
    "        self.pos = pos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.words[idx], self.pos[idx], self.labels[idx]\n",
    "    \n",
    "# Sorting by length (this is necessary for pack_padded_sequence)\n",
    "def collate_fn(batch):\n",
    "    words, pos, labels = zip(*batch)\n",
    "    \n",
    "    # Sorting by length of the words (descending order)\n",
    "    lengths = torch.tensor([len(w) for w in words])\n",
    "    sorted_idx = torch.argsort(lengths, descending=True)\n",
    "    \n",
    "    words = [words[i] for i in sorted_idx]\n",
    "    pos = [pos[i] for i in sorted_idx]\n",
    "    labels = [labels[i] for i in sorted_idx]\n",
    "    lengths = lengths[sorted_idx]\n",
    "\n",
    "    # Padding the sequences\n",
    "    words_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(w) for w in words], batch_first=True, padding_value=0)\n",
    "    pos_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(p) for p in pos], batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(l) for l in labels], batch_first=True, padding_value=0)\n",
    "\n",
    "    return words_padded, pos_padded, labels_padded, lengths\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = NERDataset(input_words, input_pos, label_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced NER Model with POS tag input\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, vocab_size, pos_size, tagset_size, \n",
    "                 embedding_dim=50, pos_embedding_dim=20, hidden_dim=100, dropout=0.3):\n",
    "        super(NERModel, self).__init__()\n",
    "        # Embedding layers for words and POS tags\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(pos_size, pos_embedding_dim)\n",
    "        \n",
    "        # Combine word and POS embeddings\n",
    "        combined_dim = embedding_dim + pos_embedding_dim\n",
    "        \n",
    "        # LSTM layer with dropout\n",
    "        self.lstm = nn.LSTM(combined_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, words, pos):\n",
    "        # Embed words and POS tags\n",
    "        word_emb = self.word_embedding(words)\n",
    "        pos_emb = self.pos_embedding(pos)\n",
    "        \n",
    "        # Concatenate word and POS embeddings\n",
    "        combined_emb = torch.cat((word_emb, pos_emb), dim=2)\n",
    "        \n",
    "        # LSTM and classification\n",
    "        lstm_out, _ = self.lstm(combined_emb)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters\n",
    "vocab_size = len(word2idx)\n",
    "pos_size = len(pos2idx)\n",
    "tagset_size = len(label2idx)\n",
    "\n",
    "# Create model\n",
    "model = NERModel(vocab_size, pos_size, tagset_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding in loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.9562294483184814\n",
      "Epoch 2, Loss: 1.9049075841903687\n",
      "Epoch 3, Loss: 1.8546748161315918\n",
      "Epoch 4, Loss: 1.8050137758255005\n",
      "Epoch 5, Loss: 1.7554737329483032\n",
      "Epoch 6, Loss: 1.7056702375411987\n",
      "Epoch 7, Loss: 1.6552882194519043\n",
      "Epoch 8, Loss: 1.6040890216827393\n",
      "Epoch 9, Loss: 1.551914095878601\n",
      "Epoch 10, Loss: 1.498694658279419\n",
      "Epoch 11, Loss: 1.4444619417190552\n",
      "Epoch 12, Loss: 1.3893502950668335\n",
      "Epoch 13, Loss: 1.3335877656936646\n",
      "Epoch 14, Loss: 1.2774722576141357\n",
      "Epoch 15, Loss: 1.2213279008865356\n",
      "Epoch 16, Loss: 1.1654452085494995\n",
      "Epoch 17, Loss: 1.110026240348816\n",
      "Epoch 18, Loss: 1.05516517162323\n",
      "Epoch 19, Loss: 1.0008927583694458\n",
      "Epoch 20, Loss: 0.947274386882782\n",
      "Epoch 21, Loss: 0.8945138454437256\n",
      "Epoch 22, Loss: 0.8429822325706482\n",
      "Epoch 23, Loss: 0.7931394577026367\n",
      "Epoch 24, Loss: 0.7453895807266235\n",
      "Epoch 25, Loss: 0.6999677419662476\n",
      "Epoch 26, Loss: 0.6569318175315857\n",
      "Epoch 27, Loss: 0.6162347793579102\n",
      "Epoch 28, Loss: 0.5778046250343323\n",
      "Epoch 29, Loss: 0.5415812134742737\n",
      "Epoch 30, Loss: 0.5075042843818665\n",
      "Epoch 31, Loss: 0.4754824936389923\n",
      "Epoch 32, Loss: 0.44537249207496643\n",
      "Epoch 33, Loss: 0.41699913144111633\n",
      "Epoch 34, Loss: 0.39021241664886475\n",
      "Epoch 35, Loss: 0.36494067311286926\n",
      "Epoch 36, Loss: 0.3411875069141388\n",
      "Epoch 37, Loss: 0.31897082924842834\n",
      "Epoch 38, Loss: 0.2982569634914398\n",
      "Epoch 39, Loss: 0.27894410490989685\n",
      "Epoch 40, Loss: 0.2609049379825592\n",
      "Epoch 41, Loss: 0.24403947591781616\n",
      "Epoch 42, Loss: 0.2282925695180893\n",
      "Epoch 43, Loss: 0.21363115310668945\n",
      "Epoch 44, Loss: 0.20001277327537537\n",
      "Epoch 45, Loss: 0.1873711198568344\n",
      "Epoch 46, Loss: 0.175624817609787\n",
      "Epoch 47, Loss: 0.16469430923461914\n",
      "Epoch 48, Loss: 0.15451209247112274\n",
      "Epoch 49, Loss: 0.1450231671333313\n",
      "Epoch 50, Loss: 0.13617943227291107\n",
      "Epoch 51, Loss: 0.1279364973306656\n",
      "Epoch 52, Loss: 0.12025154381990433\n",
      "Epoch 53, Loss: 0.11308365315198898\n",
      "Epoch 54, Loss: 0.10639461874961853\n",
      "Epoch 55, Loss: 0.10015027970075607\n",
      "Epoch 56, Loss: 0.09432011097669601\n",
      "Epoch 57, Loss: 0.08887746930122375\n",
      "Epoch 58, Loss: 0.08379844576120377\n",
      "Epoch 59, Loss: 0.07906126976013184\n",
      "Epoch 60, Loss: 0.07464528828859329\n",
      "Epoch 61, Loss: 0.0705304890871048\n",
      "Epoch 62, Loss: 0.06669750809669495\n",
      "Epoch 63, Loss: 0.06312749534845352\n",
      "Epoch 64, Loss: 0.059802282601594925\n",
      "Epoch 65, Loss: 0.05670478194952011\n",
      "Epoch 66, Loss: 0.05381901562213898\n",
      "Epoch 67, Loss: 0.05113007128238678\n",
      "Epoch 68, Loss: 0.04862426593899727\n",
      "Epoch 69, Loss: 0.046288762241601944\n",
      "Epoch 70, Loss: 0.04411159083247185\n",
      "Epoch 71, Loss: 0.04208158701658249\n",
      "Epoch 72, Loss: 0.04018821939826012\n",
      "Epoch 73, Loss: 0.038421645760536194\n",
      "Epoch 74, Loss: 0.0367724746465683\n",
      "Epoch 75, Loss: 0.035232022404670715\n",
      "Epoch 76, Loss: 0.03379201889038086\n",
      "Epoch 77, Loss: 0.03244493901729584\n",
      "Epoch 78, Loss: 0.031183697283267975\n",
      "Epoch 79, Loss: 0.03000173158943653\n",
      "Epoch 80, Loss: 0.028893064707517624\n",
      "Epoch 81, Loss: 0.027852095663547516\n",
      "Epoch 82, Loss: 0.02687389776110649\n",
      "Epoch 83, Loss: 0.025953728705644608\n",
      "Epoch 84, Loss: 0.025087391957640648\n",
      "Epoch 85, Loss: 0.02427087351679802\n",
      "Epoch 86, Loss: 0.023500610142946243\n",
      "Epoch 87, Loss: 0.02277325466275215\n",
      "Epoch 88, Loss: 0.02208579331636429\n",
      "Epoch 89, Loss: 0.02143533155322075\n",
      "Epoch 90, Loss: 0.02081928960978985\n",
      "Epoch 91, Loss: 0.020235272124409676\n",
      "Epoch 92, Loss: 0.019681043922901154\n",
      "Epoch 93, Loss: 0.019154513254761696\n",
      "Epoch 94, Loss: 0.01865389011800289\n",
      "Epoch 95, Loss: 0.018177323043346405\n",
      "Epoch 96, Loss: 0.017723316326737404\n",
      "Epoch 97, Loss: 0.017290396615862846\n",
      "Epoch 98, Loss: 0.016877025365829468\n",
      "Epoch 99, Loss: 0.016482165083289146\n",
      "Epoch 100, Loss: 0.01610453799366951\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):  # for simplicity, we use 10 epochs\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for words, pos, labels, lengths in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(words, pos)\n",
    "\n",
    "        # Flatten the outputs and labels for the loss function\n",
    "        outputs = outputs.view(-1, tagset_size)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Compute loss, for all tokens (including padding)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function\n",
    "def predict(sentence):\n",
    "    # Use Stanza for POS tagging\n",
    "    doc = nlp(\" \".join(sentence))\n",
    "    \n",
    "    # Extract words and POS tags\n",
    "    processed_words = []\n",
    "    processed_pos = []\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            processed_words.append(word.text)\n",
    "            processed_pos.append(word.upos)\n",
    "    \n",
    "    # Convert to indices, handling out-of-vocabulary words\n",
    "    input_words = [word2idx.get(word, word2idx[\"<PAD>\"]) for word in processed_words]\n",
    "    input_pos = [pos2idx.get(pos, pos2idx[\"<PAD>\"]) for pos in processed_pos]\n",
    "\n",
    "    print(input_pos)\n",
    "    \n",
    "    # Pad sequences\n",
    "    input_words = input_words + [0] * (MAX_LEN - len(input_words))\n",
    "    input_pos = input_pos + [0] * (MAX_LEN - len(input_pos))\n",
    "    \n",
    "    # Convert to tensors\n",
    "    input_words = torch.tensor([input_words]).long()\n",
    "    input_pos = torch.tensor([input_pos]).long()\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_words, input_pos)\n",
    "        _, predicted = torch.max(outputs, dim=2)\n",
    "\n",
    "    print(outputs)\n",
    "    \n",
    "    # Convert indices back to labels\n",
    "    predicted_labels = [list(label2idx.keys())[i] for i in predicted[0]]\n",
    "\n",
    "    # Truncate to original sentence length\n",
    "    predicted_labels = predicted_labels[:len(processed_words)]\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 2, 3, 3, 7, 3, 8, 6, 7, 3, 2, 2]\n",
      "tensor([[[-0.6450, -0.3318, -0.2371,  0.9593,  0.4024, -0.9449, -0.6222],\n",
      "         [-1.4061, -2.0003, -0.7597,  5.2985,  0.6206, -2.0661, -1.7204],\n",
      "         [-2.4557, -2.7283, -1.0173,  0.5376,  5.6073, -1.4381, -1.2368],\n",
      "         [-2.1081, -2.8489, -2.1193,  0.5886,  4.2446,  1.0029, -0.8788],\n",
      "         [-1.4612, -1.6668, -2.7151, -0.8080,  2.5049,  2.5858, -0.3093],\n",
      "         [-0.9785, -1.6390, -2.2709,  0.3266,  1.3683,  0.9302,  1.0329],\n",
      "         [-1.3084, -1.8412, -2.0618,  0.9541,  0.9491,  0.0946,  0.9662],\n",
      "         [-1.3093, -1.8284, -2.0625,  1.9060,  0.9044, -0.4293,  0.5342],\n",
      "         [-1.3405, -1.0176, -2.1933,  2.8393,  0.8701, -0.3405, -0.6533],\n",
      "         [-1.4826, -0.6979, -2.7498,  3.4376,  0.6182, -0.6211, -1.4563],\n",
      "         [-1.9147, -1.4211, -1.6628,  4.3297,  0.6069, -1.1083, -2.1393],\n",
      "         [-2.0505, -2.4103, -1.5526,  6.5060,  0.9951, -1.8541, -2.2841],\n",
      "         [-2.9026, -3.1289, -1.7211,  0.7781,  6.0488, -1.1341, -1.4557]]])\n",
      "Predicted Labels: ['B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-LEAGUE', 'I-ORG', 'I-LEAGUE', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'I-ORG']\n",
      "\n",
      "Word to Index: {'<PAD>': 0, 'Ã¯Â»': 1, 'Â¿Luis': 2, 'Enrique': 3, 'tahu': 4, 'start': 5, 'Paris': 6, 'Saint': 7, 'Germain': 8, 'di': 9, 'Liga': 10, 'Champions': 11, 'tidak': 12, 'bagus': 13, 'Maka': 14, 'Les': 15, 'Parisiens': 16, 'dituntut': 17, 'bangkit': 18, 'meski': 19, 'punya': 20, 'catatan': 21, 'negatif': 22, 'saat': 23, 'bertemu': 24, 'Bayern': 25, 'Munich': 26}\n",
      "\n",
      "POS to Index: {'<PAD>': 0, 'PUNCT': 1, 'PROPN': 2, 'VERB': 3, 'ADP': 4, 'PART': 5, 'ADJ': 6, 'SCONJ': 7, 'NOUN': 8}\n",
      "\n",
      "Label to Index: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LEAGUE': 5, 'I-LEAGUE': 6}\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_sentence = [\"Maka\", \"Les\", \"Parisiens\", \"dituntut\", \"bangkit\", \"meski\", \"punya\", \"catatan\", \"negatif\", \"saat\", \"bertemu\", \"Bayern\", \"Munich\"]\n",
    "predictions = predict(test_sentence)\n",
    "print(\"Predicted Labels:\", predictions)\n",
    "\n",
    "# Print out vocabularies for reference\n",
    "print(\"\\nWord to Index:\", dict(word2idx))\n",
    "print(\"\\nPOS to Index:\", dict(pos2idx))\n",
    "print(\"\\nLabel to Index:\", dict(label2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 22:54:27 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42908f0cf1941cabfc1d24ffe227733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 22:54:27 INFO: Downloaded file to C:\\Users\\jonat\\stanza_resources\\resources.json\n",
      "2024-11-27 22:54:27 WARNING: Language id package default expects mwt, which has been added\n",
      "2024-11-27 22:54:28 INFO: Loading these models for language: id (Indonesian):\n",
      "==========================\n",
      "| Processor | Package    |\n",
      "--------------------------\n",
      "| tokenize  | gsd        |\n",
      "| mwt       | gsd        |\n",
      "| pos       | gsd_charlm |\n",
      "==========================\n",
      "\n",
      "2024-11-27 22:54:28 INFO: Using device: cuda\n",
      "2024-11-27 22:54:28 INFO: Loading: tokenize\n",
      "2024-11-27 22:54:28 INFO: Loading: mwt\n",
      "2024-11-27 22:54:28 INFO: Loading: pos\n",
      "2024-11-27 22:54:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ã¯Â»Â¿Luis Enrique tahu start Paris Saint Germain di Liga Champions tidak bagus', 'B-PER I-PER O O B-ORG I-ORG I-ORG O B-LEAGUE I-LEAGUE O O']\n",
      "['Maka Les Parisiens dituntut bangkit meski punya catatan negatif saat bertemu Bayern Munich', 'O B-ORG I-ORG O O O O O O O O B-ORG I-ORG']\n",
      "[['Ã¯Â»Â¿Luis', 'Enrique', 'tahu', 'start', 'Paris', 'Saint', 'Germain', 'di', 'Liga', 'Champions', 'tidak', 'bagus'], ['Maka', 'Les', 'Parisiens', 'dituntut', 'bangkit', 'meski', 'punya', 'catatan', 'negatif', 'saat', 'bertemu', 'Bayern', 'Munich']]\n",
      "Epoch 1, Loss: 1.9498374462127686\n",
      "Epoch 2, Loss: 1.8568024635314941\n",
      "Epoch 3, Loss: 1.7719604969024658\n",
      "Epoch 4, Loss: 1.694122314453125\n",
      "Epoch 5, Loss: 1.622365951538086\n",
      "Epoch 6, Loss: 1.5561259984970093\n",
      "Epoch 7, Loss: 1.4950679540634155\n",
      "Epoch 8, Loss: 1.438909649848938\n",
      "Epoch 9, Loss: 1.3872342109680176\n",
      "Epoch 10, Loss: 1.3393231630325317\n",
      "Epoch 11, Loss: 1.294113278388977\n",
      "Epoch 12, Loss: 1.2503514289855957\n",
      "Epoch 13, Loss: 1.20686674118042\n",
      "Epoch 14, Loss: 1.162819743156433\n",
      "Epoch 15, Loss: 1.117836833000183\n",
      "Epoch 16, Loss: 1.072020411491394\n",
      "Epoch 17, Loss: 1.025833010673523\n",
      "Epoch 18, Loss: 0.9798741936683655\n",
      "Epoch 19, Loss: 0.934671938419342\n",
      "Epoch 20, Loss: 0.8905696868896484\n",
      "Epoch 21, Loss: 0.8476480841636658\n",
      "Epoch 22, Loss: 0.8056638836860657\n",
      "Epoch 23, Loss: 0.7641544342041016\n",
      "Epoch 24, Loss: 0.7227925062179565\n",
      "Epoch 25, Loss: 0.681624174118042\n",
      "Epoch 26, Loss: 0.641011655330658\n",
      "Epoch 27, Loss: 0.6014562249183655\n",
      "Epoch 28, Loss: 0.5634151101112366\n",
      "Epoch 29, Loss: 0.527183473110199\n",
      "Epoch 30, Loss: 0.4928966760635376\n",
      "Epoch 31, Loss: 0.4606057107448578\n",
      "Epoch 32, Loss: 0.43031570315361023\n",
      "Epoch 33, Loss: 0.4019624590873718\n",
      "Epoch 34, Loss: 0.3753848969936371\n",
      "Epoch 35, Loss: 0.3503532409667969\n",
      "Epoch 36, Loss: 0.3266644775867462\n",
      "Epoch 37, Loss: 0.3042363226413727\n",
      "Epoch 38, Loss: 0.28310278058052063\n",
      "Epoch 39, Loss: 0.2633131444454193\n",
      "Epoch 40, Loss: 0.24486078321933746\n",
      "Epoch 41, Loss: 0.22771476209163666\n",
      "Epoch 42, Loss: 0.2118706852197647\n",
      "Epoch 43, Loss: 0.19732566177845\n",
      "Epoch 44, Loss: 0.18400755524635315\n",
      "Epoch 45, Loss: 0.17175374925136566\n",
      "Epoch 46, Loss: 0.1603775918483734\n",
      "Epoch 47, Loss: 0.1497543752193451\n",
      "Epoch 48, Loss: 0.13984861969947815\n",
      "Epoch 49, Loss: 0.13066735863685608\n",
      "Epoch 50, Loss: 0.12220314145088196\n",
      "Epoch 51, Loss: 0.11441703140735626\n",
      "Epoch 52, Loss: 0.10725624114274979\n",
      "Epoch 53, Loss: 0.10067253559827805\n",
      "Epoch 54, Loss: 0.09462528675794601\n",
      "Epoch 55, Loss: 0.08907481282949448\n",
      "Epoch 56, Loss: 0.08397777378559113\n",
      "Epoch 57, Loss: 0.07928891479969025\n",
      "Epoch 58, Loss: 0.07496628165245056\n",
      "Epoch 59, Loss: 0.07097475230693817\n",
      "Epoch 60, Loss: 0.06728542596101761\n",
      "Epoch 61, Loss: 0.06387347728013992\n",
      "Epoch 62, Loss: 0.060715362429618835\n",
      "Epoch 63, Loss: 0.05778837576508522\n",
      "Epoch 64, Loss: 0.05507107824087143\n",
      "Epoch 65, Loss: 0.05254468694329262\n",
      "Epoch 66, Loss: 0.050192590802907944\n",
      "Epoch 67, Loss: 0.04800069332122803\n",
      "Epoch 68, Loss: 0.045956406742334366\n",
      "Epoch 69, Loss: 0.04404807090759277\n",
      "Epoch 70, Loss: 0.042264729738235474\n",
      "Epoch 71, Loss: 0.04059605300426483\n",
      "Epoch 72, Loss: 0.03903266042470932\n",
      "Epoch 73, Loss: 0.037566039711236954\n",
      "Epoch 74, Loss: 0.03618871793150902\n",
      "Epoch 75, Loss: 0.03489379957318306\n",
      "Epoch 76, Loss: 0.03367527574300766\n",
      "Epoch 77, Loss: 0.0325273759663105\n",
      "Epoch 78, Loss: 0.0314447283744812\n",
      "Epoch 79, Loss: 0.030422495678067207\n",
      "Epoch 80, Loss: 0.02945592813193798\n",
      "Epoch 81, Loss: 0.028541041538119316\n",
      "Epoch 82, Loss: 0.027673928067088127\n",
      "Epoch 83, Loss: 0.026851290836930275\n",
      "Epoch 84, Loss: 0.02607002854347229\n",
      "Epoch 85, Loss: 0.025327444076538086\n",
      "Epoch 86, Loss: 0.02462099678814411\n",
      "Epoch 87, Loss: 0.023948390036821365\n",
      "Epoch 88, Loss: 0.0233074352145195\n",
      "Epoch 89, Loss: 0.0226961188018322\n",
      "Epoch 90, Loss: 0.022112548351287842\n",
      "Epoch 91, Loss: 0.021555107086896896\n",
      "Epoch 92, Loss: 0.02102205902338028\n",
      "Epoch 93, Loss: 0.02051200345158577\n",
      "Epoch 94, Loss: 0.020023614168167114\n",
      "Epoch 95, Loss: 0.019555533304810524\n",
      "Epoch 96, Loss: 0.019106633961200714\n",
      "Epoch 97, Loss: 0.018675824627280235\n",
      "Epoch 98, Loss: 0.018262051045894623\n",
      "Epoch 99, Loss: 0.017864344641566277\n",
      "Epoch 100, Loss: 0.01748174987733364\n",
      "Predicted Labels: ['B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'B-ORG', 'I-ORG']\n",
      "\n",
      "POS to Index: {'<PAD>': 0, 'PUNCT': 1, 'PROPN': 2, 'VERB': 3, 'ADP': 4, 'PART': 5, 'ADJ': 6, 'SCONJ': 7, 'NOUN': 8}\n",
      "\n",
      "Label to Index: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LEAGUE': 5, 'I-LEAGUE': 6}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import stanza\n",
    "\n",
    "def extract_text(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        train = f.read().split(\"\\n\\n\")\n",
    "        train = [x.split(\"\\n\") for x in train]\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        print(train[i])\n",
    "        sentence = train[i][0].split(\" \")\n",
    "        label = train[i][1].split(\" \")\n",
    "        if len(sentence) == len(label):\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "# Initialize Stanza NLP pipeline\n",
    "nlp = stanza.Pipeline('id', processors='tokenize,pos')\n",
    "\n",
    "# Data Sample\n",
    "sentences, labels = extract_text(\"TAGGED REVISI DIKIT.txt\")\n",
    "print(sentences)\n",
    "\n",
    "# Create Vocabulary for POS tags and labels\n",
    "pos2idx = defaultdict(lambda: len(pos2idx))\n",
    "label2idx = defaultdict(lambda: len(label2idx))\n",
    "\n",
    "# Add special tokens\n",
    "pos2idx[\"<PAD>\"] = 0\n",
    "label2idx[\"O\"] = 0\n",
    "\n",
    "# Preprocess training data for initial vocabulary\n",
    "def preprocess_training_data(sentences, labels):\n",
    "    processed_data = []\n",
    "    for sentence, sent_labels in zip(sentences, labels):\n",
    "        # Use Stanza for POS tagging\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "        \n",
    "        processed_pos = []\n",
    "        processed_labels = []\n",
    "        \n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                processed_pos.append(word.upos)\n",
    "        \n",
    "        # Match labels to processed POS tags (assuming same order)\n",
    "        processed_labels = sent_labels[:len(processed_pos)]\n",
    "        \n",
    "        processed_data.append((processed_pos, processed_labels))\n",
    "        \n",
    "        # Update vocabularies\n",
    "        for pos in processed_pos:\n",
    "            pos2idx[pos]\n",
    "        for label in processed_labels:\n",
    "            label2idx[label]\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Preprocess the initial training data\n",
    "processed_training_data = preprocess_training_data(sentences, labels)\n",
    "\n",
    "# Convert POS tags and labels to indices\n",
    "input_pos = [[pos2idx[pos] for pos in sentence[0]] for sentence in processed_training_data]\n",
    "label_data = [[label2idx[label] for label in sentence[1]] for sentence in processed_training_data]\n",
    "\n",
    "# Dynamic Padding: Use the length of the longest sentence in the data\n",
    "MAX_LEN = max([len(sentence) for sentence in input_pos])\n",
    "\n",
    "# Padding the sequences\n",
    "input_pos = [pos + [0]*(MAX_LEN - len(pos)) for pos in input_pos]\n",
    "label_data = [label + [0]*(MAX_LEN - len(label)) for label in label_data]\n",
    "\n",
    "# Dataset Class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, pos, labels):\n",
    "        self.pos = pos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos[idx], self.labels[idx]\n",
    "    \n",
    "# Sorting by length (this is necessary for pack_padded_sequence)\n",
    "def collate_fn(batch):\n",
    "    pos, labels = zip(*batch)\n",
    "    \n",
    "    # Sorting by length of the POS tags (descending order)\n",
    "    lengths = torch.tensor([len(p) for p in pos])\n",
    "    sorted_idx = torch.argsort(lengths, descending=True)\n",
    "    \n",
    "    pos = [pos[i] for i in sorted_idx]\n",
    "    labels = [labels[i] for i in sorted_idx]\n",
    "    lengths = lengths[sorted_idx]\n",
    "\n",
    "    # Padding the sequences\n",
    "    pos_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(p) for p in pos], batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(l) for l in labels], batch_first=True, padding_value=0)\n",
    "\n",
    "    return pos_padded, labels_padded, lengths\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = NERDataset(input_pos, label_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Enhanced NER Model with POS tag input\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, pos_size, tagset_size, pos_embedding_dim=100, hidden_dim=100, dropout=0):\n",
    "        super(NERModel, self).__init__()\n",
    "        # Embedding layer for POS tags\n",
    "        self.pos_embedding = nn.Embedding(pos_size, pos_embedding_dim)\n",
    "        \n",
    "        # LSTM layer with dropout\n",
    "        self.lstm = nn.LSTM(pos_embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, pos):\n",
    "        # Embed POS tags\n",
    "        pos_emb = self.pos_embedding(pos)\n",
    "        \n",
    "        # LSTM and classification\n",
    "        lstm_out, _ = self.lstm(pos_emb)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "# Set model parameters\n",
    "pos_size = len(pos2idx)\n",
    "tagset_size = len(label2idx)\n",
    "\n",
    "# Create model\n",
    "model = NERModel(pos_size, tagset_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding in loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # for simplicity, we use 100 epochs\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for pos, labels, lengths in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(pos)\n",
    "\n",
    "        # Flatten the outputs and labels for the loss function\n",
    "        outputs = outputs.view(-1, tagset_size)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Create mask to ignore padding tokens in the loss calculation\n",
    "        mask = labels != 0  # Only valid tokens, ignore padding (0)\n",
    "\n",
    "        # Check if there are any valid tokens to compute loss\n",
    "        if mask.sum() == 0:  # No valid tokens to calculate loss\n",
    "            continue\n",
    "        \n",
    "        # Apply the mask to the outputs and labels (flattened)\n",
    "        outputs = outputs[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "        # Compute loss, only for valid tokens (non-padding)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "# Predict function (using only POS tags as input)\n",
    "def predict(sentence):\n",
    "    # Use Stanza for POS tagging\n",
    "    doc = nlp(\" \".join(sentence))\n",
    "    \n",
    "    # Extract POS tags\n",
    "    processed_pos = [word.upos for sent in doc.sentences for word in sent.words]\n",
    "    \n",
    "    # Convert to indices, handling out-of-vocabulary POS tags\n",
    "    input_pos = [pos2idx.get(pos, pos2idx[\"<PAD>\"]) for pos in processed_pos]\n",
    "    \n",
    "    # Pad sequences\n",
    "    input_pos = input_pos + [0] * (MAX_LEN - len(input_pos))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_pos = torch.tensor([input_pos]).long()\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_pos)\n",
    "        _, predicted = torch.max(outputs, dim=2)\n",
    "\n",
    "    # Convert indices back to labels\n",
    "    predicted_labels = [list(label2idx.keys())[i] for i in predicted[0]]\n",
    "\n",
    "    # Truncate to original sentence length\n",
    "    predicted_labels = predicted_labels[:len(processed_pos)]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "# Test the model\n",
    "test_sentence = [\"Maka\", \"Les\", \"Parisiens\", \"dituntut\", \"bangkit\", \"meski\", \"punya\", \"catatan\", \"negatif\", \"saat\", \"bertemu\", \"Bayern\", \"Munich\"]\n",
    "predictions = predict(test_sentence)\n",
    "print(\"Predicted Labels:\", predictions)\n",
    "\n",
    "# Print out vocabularies for reference\n",
    "print(\"\\nPOS to Index:\", dict(pos2idx))\n",
    "print(\"\\nLabel to Index:\", dict(label2idx))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
